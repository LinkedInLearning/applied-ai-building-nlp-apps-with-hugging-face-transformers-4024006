{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f948d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "#Set to avoid warning messages.\n",
    "transformers.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eca074",
   "metadata": {},
   "source": [
    "## 04.02. Loading a Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a6b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 892/892 [00:00<00:00, 247229.18 examples/s]\n",
      "Generating validation split: 100%|██████████| 105/105 [00:00<00:00, 61620.53 examples/s]\n",
      "Generating test split: 100%|██████████| 104/104 [00:00<00:00, 70905.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 105\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "})\n",
      "{'id': [20, 21, 22, 23, 24], 'verse_text': [\"as o'er the earth it wanders wide,\", 'how hearts were answering to his own,', 'glad on its stone-built hearth; and thorough the wide-mouthed smoke-flue', 'sees the clouds reel and roll above our head,', '’tis to behold his vengeance for my son.'], 'label': [2, 1, 2, 2, 0]}\n",
      "\n",
      "Sentiment Labels used ['negative', 'positive', 'no_impact', 'mixed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Use pretrained model checkpoint from Huggingface\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "#Use pre-labeled dataset from huggingface\n",
    "dataset_name= \"poem_sentiment\"\n",
    "\n",
    "poem_sentiments = load_dataset(dataset_name)\n",
    "\n",
    "#Apache Arrow format\n",
    "print(poem_sentiments)\n",
    "print(poem_sentiments[\"test\"][20:25])\n",
    "\n",
    "print(\"\\nSentiment Labels used\", \n",
    "      poem_sentiments[\"train\"].features.get(\"label\").names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54018a23",
   "metadata": {},
   "source": [
    "## 04.03. Encoding and pre-processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb057a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 892/892 [00:00<00:00, 5169.15 examples/s]\n",
      "Map: 100%|██████████| 105/105 [00:00<00:00, 4360.11 examples/s]\n",
      "Map: 100%|██████████| 104/104 [00:00<00:00, 4546.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [0, 1, 2, 3, 4], 'verse_text': ['with pale blue berries. in these peaceful shades--', 'it flows so long as falls the rain,', 'and that is why, the lonesome day,', 'when i peruse the conquered fame of heroes, and the victories of mighty generals, i do not envy the generals,', 'of inward strife for truth and liberty.'], 'label': [1, 2, 0, 3, 3], 'input_ids': [[101, 2007, 5122, 2630, 22681, 1012, 1999, 2122, 9379, 13178, 1011, 1011, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2009, 6223, 2061, 2146, 2004, 4212, 1996, 4542, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1998, 2008, 2003, 2339, 1010, 1996, 10459, 14045, 2154, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1045, 7304, 3366, 1996, 11438, 4476, 1997, 7348, 1010, 1998, 1996, 9248, 1997, 10478, 11593, 1010, 1045, 2079, 2025, 21103, 1996, 11593, 1010, 102, 0, 0], [101, 1997, 20546, 27865, 2005, 3606, 1998, 7044, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Encoding text\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "db_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return db_tokenizer(batch[\"verse_text\"], \n",
    "                        padding=True, \n",
    "                        truncation=True)\n",
    "\n",
    "enc_poem_sentiment = poem_sentiments.map(\n",
    "                        tokenize, \n",
    "                        batched=True, \n",
    "                        batch_size=None)\n",
    "\n",
    "print(enc_poem_sentiment[\"train\"][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5775b7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : it flows so long as falls the rain,\n",
      "\n",
      "Input Map : [101, 2009, 6223, 2061, 2146, 2004, 4212, 1996, 4542, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Attention Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Total tokens:  28\n",
      "Non Zero tokens:  11\n",
      "Attention = 1:  11\n"
     ]
    }
   ],
   "source": [
    "#Explore input IDs and Attention Mask\n",
    "\n",
    "print(\"Text :\", enc_poem_sentiment[\"train\"][1].get(\"verse_text\"))\n",
    "print(\"\\nInput Map :\", enc_poem_sentiment[\"train\"][1].get(\"input_ids\"))\n",
    "print(\"\\nAttention Mask :\", enc_poem_sentiment[\"train\"][1].get(\"attention_mask\"))\n",
    "\n",
    "print(\"\\nTotal tokens: \", len(enc_poem_sentiment[\"train\"][1].get(\"input_ids\")))\n",
    "print(\"Non Zero tokens: \", len(list(filter( \n",
    "    lambda x :x > 0, enc_poem_sentiment[\"train\"][1].get(\"input_ids\")))))\n",
    "print(\"Attention = 1: \", len(list(filter( \n",
    "    lambda x :x > 0, enc_poem_sentiment[\"train\"][1].get(\"attention_mask\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a924c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Names :  ['id', 'verse_text', 'label', 'input_ids', 'attention_mask']\n",
      "\n",
      "Features :  {'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "#Separate training and validation sets\n",
    "training_dataset = enc_poem_sentiment[\"train\"]\n",
    "validation_dataset=enc_poem_sentiment[\"validation\"]\n",
    "\n",
    "print(\"\\nColumn Names : \",training_dataset.column_names)\n",
    "print(\"\\nFeatures : \",training_dataset.features)\n",
    "\n",
    "labels = training_dataset.features.get(\"label\")\n",
    "num_labels=len(labels.names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3faefe9",
   "metadata": {},
   "source": [
    "## 04.04. Creating the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2094ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 21:06:17.624294: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-26 21:06:17.627600: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-26 21:06:17.635091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750971977.649268    6133 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750971977.653549    6133 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750971977.665619    6133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750971977.665629    6133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750971977.665631    6133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750971977.665633    6133 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-26 21:06:17.669719: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-26 21:06:21.987868: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-26 21:06:22.056417: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2025-06-26 21:06:22.165410: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2025-06-26 21:06:22.174706: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2025-06-26 21:06:22.682509: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 30522,\n",
       " 'max_position_embeddings': 512,\n",
       " 'sinusoidal_pos_embds': False,\n",
       " 'n_layers': 6,\n",
       " 'n_heads': 12,\n",
       " 'dim': 768,\n",
       " 'hidden_dim': 3072,\n",
       " 'dropout': 0.1,\n",
       " 'attention_dropout': 0.1,\n",
       " 'activation': 'gelu',\n",
       " 'initializer_range': 0.02,\n",
       " 'qa_dropout': 0.1,\n",
       " 'seq_classif_dropout': 0.2,\n",
       " 'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'torchscript': False,\n",
       " 'torch_dtype': None,\n",
       " 'use_bfloat16': False,\n",
       " 'tf_legacy_loss': False,\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': True,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'architectures': ['DistilBertForMaskedLM'],\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3},\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': None,\n",
       " 'pad_token_id': 0,\n",
       " 'eos_token_id': None,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " '_name_or_path': 'distilbert-base-uncased',\n",
       " 'transformers_version': '4.53.0',\n",
       " 'model_type': 'distilbert',\n",
       " 'tie_weights_': True,\n",
       " 'output_attentions': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "#Load transformer checkpoint from huggingface\n",
    "sentiment_model = (TFAutoModelForSequenceClassification\n",
    "            .from_pretrained(model_name, num_labels=num_labels))\n",
    "\n",
    "sentiment_model.get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07af1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66956548 (255.42 MB)\n",
      "Trainable params: 66956548 (255.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Freeze the first layer if needed\n",
    "sentiment_model.layers[0].trainable = True\n",
    "\n",
    "#Add/remove layers if needed.\n",
    "#sentiment_model.layers [append()/insert()/remove()]\n",
    "\n",
    "print(sentiment_model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575dca13",
   "metadata": {},
   "source": [
    "# 04.05. Training the Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72bdd464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 21:06:27.133806: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 39s 2s/step - loss: 1.1322 - sparse_categorical_accuracy: 0.5886 - val_loss: 0.9094 - val_sparse_categorical_accuracy: 0.6571\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 26s 2s/step - loss: 0.9199 - sparse_categorical_accuracy: 0.6300 - val_loss: 0.6618 - val_sparse_categorical_accuracy: 0.8571\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 26s 2s/step - loss: 0.5717 - sparse_categorical_accuracy: 0.8442 - val_loss: 0.4856 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 27s 2s/step - loss: 0.2819 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.4851 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 25s 2s/step - loss: 0.1735 - sparse_categorical_accuracy: 0.9451 - val_loss: 0.5538 - val_sparse_categorical_accuracy: 0.8286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7267181a56d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using features from a pretrained model\n",
    "\n",
    "batch_size=64\n",
    "tokenizer_columns = db_tokenizer.model_input_names\n",
    "\n",
    "# The column names to convert to TF tensors\n",
    "train_dataset = training_dataset.to_tf_dataset(\n",
    "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "val_dataset = validation_dataset.to_tf_dataset(\n",
    "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=False,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sentiment_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "sentiment_model.fit(train_dataset, validation_data=val_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86c123",
   "metadata": {},
   "source": [
    "## 04.06. Predicting Sentiment with the Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de686bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    infer: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 455.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec={'input_ids': TensorSpec(shape=(None, 17), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 17), dtype=tf.int64, name=None)}>\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "\n",
    "#Input data for interference to predict sentiment\n",
    "# the \"label\" array is not needed for inference, but added to provide true labels for comparison\n",
    "infer_data = {'id':[0,1], \n",
    "             'verse_text':['and be glad in the summer morning when the kindred ride on their way', \n",
    "                           'how hearts were answering to his own'],\n",
    "             'label':[1,0]}\n",
    "\n",
    "infer_dataset = Dataset.from_dict(infer_data)\n",
    "\n",
    "ds_dict=DatasetDict()\n",
    "ds_dict[\"infer\"] = infer_dataset\n",
    "\n",
    "print(ds_dict)\n",
    "\n",
    "#Encode the dataset, similar to training\n",
    "enc_dataset=ds_dict.map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "#Convert to Tensors\n",
    "infer_final_dataset = enc_dataset[\"infer\"].to_tf_dataset(\n",
    "    columns=tokenizer_columns,  shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "print(infer_final_dataset)\n",
    "\n",
    "#Predict with the model\n",
    "predictions=sentiment_model.predict(infer_final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2531d52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6812866 ,  2.474289  , -0.84737706, -0.25531378],\n",
       "       [-1.5751283 ,  2.5304136 , -1.2026411 , -0.17950436]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67aef028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem = and be glad in the summer morning when the kindred ride on their way  Predicted= positive  True-Label= positive\n",
      "Poem = how hearts were answering to his own  Predicted= positive  True-Label= negative\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_label_ids=np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "for i in range(len(pred_label_ids)):\n",
    "    print(\"Poem =\", infer_data[\"verse_text\"][i], \n",
    "          \" Predicted=\",labels.names[pred_label_ids[i]], \n",
    "          \" True-Label=\",labels.names[infer_data[\"label\"][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3849e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
